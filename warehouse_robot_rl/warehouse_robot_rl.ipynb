{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Warehouse Robot Navigation Using Q-Learning\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook implements a reinforcement learning solution to the warehouse robot navigation problem using Q-learning. The robot must navigate from a loading bay to a target shelf on a slippery floor while avoiding hazards (holes).\n",
        "\n",
        "## Problem Statement\n",
        "\n",
        "We use the FrozenLake-v1 environment from Gymnasium, which models:\n",
        "\n",
        "- **Environment**: A slippery warehouse floor represented as a grid\n",
        "- **Agent**: A warehouse robot that can move in 4 directions (Left, Down, Right, Up)\n",
        "- **Goal**: Navigate from start (S) to goal (G) while avoiding holes (H)\n",
        "- **Challenge**: Stochastic transitions due to slippery surface (actions may slip)\n",
        "\n",
        "## Tasks\n",
        "\n",
        "1. Understanding the Environment\n",
        "2. Setting Up the Q-Learning Agent\n",
        "3. Training the Agent\n",
        "4. Evaluation & Comparison with Baselines\n",
        "5. Hyperparameter Optimization\n",
        "6. Testing on Larger Maps (8Ã—8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All libraries imported successfully!\n",
            "Gymnasium version: 1.2.1\n",
            "NumPy version: 2.2.6\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import time\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"Gymnasium version: {gym.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1: Understanding the Environment\n",
        "\n",
        "In this section, we explore the FrozenLake-v1 environment to understand:\n",
        "\n",
        "- State space (observation space)\n",
        "- Action space\n",
        "- Grid layout (Start, Goal, Holes, Frozen tiles)\n",
        "- Reward structure\n",
        "- Effect of slippery=True (stochastic transitions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Create FrozenLake Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "FROZENLAKE ENVIRONMENT CREATED\n",
            "============================================================\n",
            "Environment: FrozenLake-v1\n",
            "Map: 4x4\n",
            "Slippery: True (stochastic transitions)\n"
          ]
        }
      ],
      "source": [
        "# Create FrozenLake environment with slippery surface\n",
        "# Starting with 4x4 map (default)\n",
        "env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=True, render_mode='ansi')\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FROZENLAKE ENVIRONMENT CREATED\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Environment: {env.spec.id}\")\n",
        "print(f\"Map: 4x4\")\n",
        "print(f\"Slippery: True (stochastic transitions)\")\n",
        "# print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Inspect State and Action Spaces\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " STATE SPACE (Observation Space):\n",
            "   Type: Discrete(16)\n",
            "   Number of states: 16\n",
            "   Description: Each tile on the grid is a discrete state (0 to 15)\n",
            "\n",
            " ACTION SPACE:\n",
            "   Type: Discrete(4)\n",
            "   Number of actions: 4\n",
            "   Actions mapping:\n",
            "      0 = LEFT\n",
            "      1 = DOWN\n",
            "      2 = RIGHT\n",
            "      3 = UP\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Print observation and action spaces\n",
        "print(\"\\n STATE SPACE (Observation Space):\")\n",
        "print(f\"   Type: {env.observation_space}\")\n",
        "print(f\"   Number of states: {env.observation_space.n}\")\n",
        "print(f\"   Description: Each tile on the grid is a discrete state (0 to {env.observation_space.n - 1})\")\n",
        "\n",
        "print(\"\\n ACTION SPACE:\")\n",
        "print(f\"   Type: {env.action_space}\")\n",
        "print(f\"   Number of actions: {env.action_space.n}\")\n",
        "print(f\"   Actions mapping:\")\n",
        "print(f\"      0 = LEFT\")\n",
        "print(f\"      1 = DOWN\")\n",
        "print(f\"      2 = RIGHT\")\n",
        "print(f\"      3 = UP\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Visualize the Grid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " GRID LAYOUT (4x4 Map):\n",
            "============================================================\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "============================================================\n",
            "\n",
            " Legend:\n",
            "   S = Start (loading bay) - Initial position\n",
            "   F = Frozen (safe tile) - Can walk on\n",
            "   H = Hole (hazard/spill) - Episode ends, reward = 0\n",
            "   G = Goal (target shelf) - Episode ends, reward = +1\n",
            "\n",
            "   The robot starts at 'S' and must reach 'G' while avoiding 'H'\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Reset environment and visualize the grid\n",
        "state, info = env.reset()\n",
        "grid_render = env.render()\n",
        "\n",
        "print(\"\\n GRID LAYOUT (4x4 Map):\")\n",
        "print(\"=\" * 60)\n",
        "print(grid_render)\n",
        "print(\"=\" * 60)\n",
        "print(\"\\n Legend:\")\n",
        "print(\"   S = Start (loading bay) - Initial position\")\n",
        "print(\"   F = Frozen (safe tile) - Can walk on\")\n",
        "print(\"   H = Hole (hazard/spill) - Episode ends, reward = 0\")\n",
        "print(\"   G = Goal (target shelf) - Episode ends, reward = +1\")\n",
        "print(\"\\n   The robot starts at 'S' and must reach 'G' while avoiding 'H'\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 Reward Structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " REWARD STRUCTURE:\n",
            "============================================================\n",
            "   Reaching Goal (G):      +1.0  (episode terminates)\n",
            "   Falling into Hole (H):   0.0  (episode terminates)\n",
            "   Safe tile (F or S):      0.0  (continue episode)\n",
            "============================================================\n",
            "\n",
            "  SPARSE REWARD CHALLENGE:\n",
            "   - Agent only gets reward when reaching the goal\n",
            "   - No intermediate feedback during navigation\n",
            "   - Must explore extensively to discover successful paths\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n REWARD STRUCTURE:\")\n",
        "print(\"=\" * 60)\n",
        "print(\"   Reaching Goal (G):      +1.0  (episode terminates)\")\n",
        "print(\"   Falling into Hole (H):   0.0  (episode terminates)\")\n",
        "print(\"   Safe tile (F or S):      0.0  (continue episode)\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\n  SPARSE REWARD CHALLENGE:\")\n",
        "print(\"   - Agent only gets reward when reaching the goal\")\n",
        "print(\"   - No intermediate feedback during navigation\")\n",
        "print(\"   - Must explore extensively to discover successful paths\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.5 Demonstration: Effect of Slippery Floor (Stochastic Transitions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " SLIPPERY FLOOR EFFECT (is_slippery=True):\n",
            "============================================================\n",
            "When the robot attempts an action, the floor is slippery!\n",
            "The actual movement has stochastic (random) transitions:\n",
            "\n",
            "   Intended direction:  33.3% chance\n",
            "   Perpendicular left:  33.3% chance\n",
            "   Perpendicular right: 33.3% chance\n",
            "\n",
            "Example: If robot tries to move RIGHT:\n",
            "   â†’ 33% moves RIGHT (intended)\n",
            "   â†’ 33% moves UP (perpendicular)\n",
            "   â†’ 33% moves DOWN (perpendicular)\n",
            "\n",
            " REAL-WORLD ANALOGY:\n",
            "   - Slippery warehouse floor with water/oil spills\n",
            "   - Wheels may slip in unexpected directions\n",
            "   - Must learn robust policy that handles uncertainty\n",
            "============================================================\n",
            "\n",
            " DEMONSTRATION: Trying to move RIGHT 10 times from start\n",
            "============================================================\n",
            "Starting state: 0 (always starts at same position)\n",
            "Action taken: RIGHT (action=2)\n",
            "\n",
            "Resulting states after action: [1, 4, 0, 4, 0, 1, 1, 1, 4, 0]\n",
            "Unique states reached: {0, 1, 4}\n",
            "\n",
            " Notice: Even with the same action, we reach different states!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n SLIPPERY FLOOR EFFECT (is_slippery=True):\")\n",
        "print(\"=\" * 60)\n",
        "print(\"When the robot attempts an action, the floor is slippery!\")\n",
        "print(\"The actual movement has stochastic (random) transitions:\\n\")\n",
        "print(\"   Intended direction:  33.3% chance\")\n",
        "print(\"   Perpendicular left:  33.3% chance\")\n",
        "print(\"   Perpendicular right: 33.3% chance\")\n",
        "print(\"\\nExample: If robot tries to move RIGHT:\")\n",
        "print(\"   â†’ 33% moves RIGHT (intended)\")\n",
        "print(\"   â†’ 33% moves UP (perpendicular)\")\n",
        "print(\"   â†’ 33% moves DOWN (perpendicular)\")\n",
        "print(\"\\n REAL-WORLD ANALOGY:\")\n",
        "print(\"   - Slippery warehouse floor with water/oil spills\")\n",
        "print(\"   - Wheels may slip in unexpected directions\")\n",
        "print(\"   - Must learn robust policy that handles uncertainty\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Demonstrate with a simple test\n",
        "print(\"\\n DEMONSTRATION: Trying to move RIGHT 10 times from start\")\n",
        "print(\"=\" * 60)\n",
        "action_right = 2  # RIGHT\n",
        "outcomes = []\n",
        "\n",
        "for i in range(10):\n",
        "    state, info = env.reset()\n",
        "    next_state, reward, terminated, truncated, info = env.step(action_right)\n",
        "    outcomes.append(next_state)\n",
        "    \n",
        "print(f\"Starting state: {state} (always starts at same position)\")\n",
        "print(f\"Action taken: RIGHT (action={action_right})\")\n",
        "print(f\"\\nResulting states after action: {outcomes}\")\n",
        "print(f\"Unique states reached: {set(outcomes)}\")\n",
        "print(\"\\n Notice: Even with the same action, we reach different states!\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2: Setting Up the RL Agent\n",
        "\n",
        "**Goal:** Teach the robot to navigate from start to goal while avoiding holes.\n",
        "\n",
        "**What it does:** Learns a Q-table that stores the value of taking each action in each state. The robot uses this table to decide which direction to move.\n",
        "\n",
        "**What it uses:**\n",
        "\n",
        "- **Q-table**: A table with rows (states) and columns (actions) storing learned values\n",
        "- **Epsilon-greedy policy**: Decision strategy that balances exploration vs exploitation\n",
        "- **Update rule**: Formula to improve Q-values based on experience\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Q-Learning Agent Implementation\n",
        "\n",
        "**Three Key Methods:**\n",
        "\n",
        "1. **select_action(state)**: Chooses which direction to move\n",
        "\n",
        "   - Flips a coin with probability epsilon\n",
        "   - If explore: pick random action\n",
        "   - If exploit: pick best action from Q-table\n",
        "\n",
        "2. **update_q_value(state, action, reward, next_state, done)**: Learns from experience\n",
        "\n",
        "   - Uses Q-learning formula to update the Q-table\n",
        "   - Increases value if action led to reward\n",
        "   - Decreases value if action was bad\n",
        "\n",
        "3. **decay_epsilon()**: Reduces exploration over time\n",
        "   - Agent explores more early in training\n",
        "   - Agent exploits learned knowledge more later in training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Q-LEARNING AGENT INITIALIZED\n",
            "============================================================\n",
            "Q-table shape: (16, 4)\n",
            "Total Q-values to learn: 64\n",
            "\n",
            "Hyperparameters:\n",
            "  Learning rate (alpha):     0.1\n",
            "  Discount factor (gamma):   0.99\n",
            "  Epsilon start:             1.0\n",
            "  Epsilon end:               0.01\n",
            "  Epsilon decay:             0.995\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "class QLearningAgent:\n",
        "    \"\"\"\n",
        "    Q-Learning Agent for warehouse robot navigation\n",
        "    \"\"\"\n",
        "    def __init__(self, env, \n",
        "                learning_rate=0.1, \n",
        "                discount_factor=0.99, \n",
        "                epsilon_start=1.0, \n",
        "                epsilon_end=0.01, \n",
        "                epsilon_decay=0.995):\n",
        "        \"\"\"\n",
        "        Initialize Q-learning agent\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.lr = learning_rate\n",
        "        self.gamma = discount_factor\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        \n",
        "        # Initialize Q-table with zeros\n",
        "        # Shape: (num_states, num_actions)\n",
        "        self.q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "        \n",
        "    def select_action(self, state):\n",
        "        \"\"\"\n",
        "        Epsilon-greedy action selection\n",
        "        - Random action with probability epsilon (explore)\n",
        "        - Best action with probability 1-epsilon (exploit)\n",
        "        \"\"\"\n",
        "        if np.random.random() < self.epsilon:\n",
        "            # Explore: random action\n",
        "            return self.env.action_space.sample()\n",
        "        else:\n",
        "            # Exploit: best action from Q-table\n",
        "            return np.argmax(self.q_table[state])\n",
        "    \n",
        "    def update_q_value(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Update Q-table using Q-learning update rule\n",
        "        Q(s,a) = Q(s,a) + alpha * [reward + gamma * max(Q(s',a')) - Q(s,a)]\n",
        "        \"\"\"\n",
        "        current_q = self.q_table[state, action]\n",
        "        \n",
        "        if done:\n",
        "            # If episode ended, no future rewards\n",
        "            target = reward\n",
        "        else:\n",
        "            # Best possible future Q-value\n",
        "            max_next_q = np.max(self.q_table[next_state])\n",
        "            target = reward + self.gamma * max_next_q\n",
        "        \n",
        "        # Update Q-table\n",
        "        self.q_table[state, action] = current_q + self.lr * (target - current_q)\n",
        "    \n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"\n",
        "        Decrease exploration rate after each episode\n",
        "        \"\"\"\n",
        "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "# Create agent instance\n",
        "agent = QLearningAgent(env)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Q-LEARNING AGENT INITIALIZED\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Q-table shape: {agent.q_table.shape}\")\n",
        "print(f\"Total Q-values to learn: {agent.q_table.size}\")\n",
        "print(f\"\\nHyperparameters:\")\n",
        "print(f\"  Learning rate (alpha):     {agent.lr}\")\n",
        "print(f\"  Discount factor (gamma):   {agent.gamma}\")\n",
        "print(f\"  Epsilon start:             {agent.epsilon}\")\n",
        "print(f\"  Epsilon end:               {agent.epsilon_end}\")\n",
        "print(f\"  Epsilon decay:             {agent.epsilon_decay}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Summary:**\n",
        "initialized with a blank Q-table (16 states Ã— 4 actions = 64 values, all zeros). The hyperparameters control how it learns: alpha (0.1) controls learning speed, gamma (0.99) values future rewards, and epsilon starts at 1.0 (full exploration) and decays to 0.01 (mostly exploitation). Next step: train the agent to fill the Q-table with useful values.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3: Training the RL Agent\n",
        "\n",
        "**Goal:** Train the agent over many episodes to learn optimal navigation policy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Training Loop Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_agent(agent,\n",
        "                 env,\n",
        "                 num_episodes=10000,\n",
        "                 max_steps=100):\n",
        "    \"\"\"\n",
        "    Train Q-learning agent over multiple episodes\n",
        "    \n",
        "    Parameters:\n",
        "    - agent: QLearningAgent instance\n",
        "    - env: Gymnasium environment\n",
        "    - num_episodes: Number of training episodes (default 10,000)\n",
        "    - max_steps: Maximum steps per episode (prevent infinite loops)\n",
        "    \n",
        "    Returns:\n",
        "    - rewards_per_episode: List of total rewards for each episode\n",
        "    - steps_per_episode: List of steps taken in each episode\n",
        "    \"\"\"\n",
        "    rewards_per_episode = []\n",
        "    steps_per_episode = []\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"TRAINING Q-LEARNING AGENT\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Episodes: {num_episodes}\")\n",
        "    print(f\"Max steps per episode: {max_steps}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        # Reset environment to start position\n",
        "        state, info = env.reset()\n",
        "        total_reward = 0\n",
        "        steps = 0\n",
        "        \n",
        "        # Run episode\n",
        "        for step in range(max_steps):\n",
        "            # Agent selects action (explore or exploit)\n",
        "            action = agent.select_action(state)\n",
        "            \n",
        "            # Take action in environment\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            \n",
        "            # Update Q-table with experience\n",
        "            agent.update_q_value(state, action, reward, next_state, done)\n",
        "            \n",
        "            # Move to next state\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            steps += 1\n",
        "            \n",
        "            # End episode if goal reached or hole fallen into\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        # Decay epsilon (reduce exploration over time)\n",
        "        agent.decay_epsilon()\n",
        "        \n",
        "        # Record episode results\n",
        "        rewards_per_episode.append(total_reward)\n",
        "        steps_per_episode.append(steps)\n",
        "        \n",
        "        # Print progress every 1000 episodes\n",
        "        if (episode + 1) % 1000 == 0:\n",
        "            avg_reward = np.mean(rewards_per_episode[-1000:])\n",
        "            avg_steps = np.mean(steps_per_episode[-1000:])\n",
        "            success_rate = np.sum(rewards_per_episode[-1000:]) / 1000 * 100\n",
        "            print(f\"Episode {episode + 1:5d} | \"\n",
        "                  f\"Avg Reward: {avg_reward:.3f} | \"\n",
        "                  f\"Avg Steps: {avg_steps:.1f} | \"\n",
        "                  f\"Success Rate: {success_rate:.1f}% | \"\n",
        "                  f\"Epsilon: {agent.epsilon:.3f}\")\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"TRAINING COMPLETE\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    return rewards_per_episode, steps_per_episode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Run Training on 4Ã—4 Map\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training on 4Ã—4 map...\n",
            "This may take 1-2 minutes...\n",
            "\n",
            "============================================================\n",
            "TRAINING Q-LEARNING AGENT\n",
            "============================================================\n",
            "Episodes: 10000\n",
            "Max steps per episode: 100\n",
            "============================================================\n",
            "Episode  1000 | Avg Reward: 0.167 | Avg Steps: 19.7 | Success Rate: 16.7% | Epsilon: 0.010\n",
            "Episode  1000 | Avg Reward: 0.167 | Avg Steps: 19.7 | Success Rate: 16.7% | Epsilon: 0.010\n",
            "Episode  2000 | Avg Reward: 0.576 | Avg Steps: 35.0 | Success Rate: 57.6% | Epsilon: 0.010\n",
            "Episode  2000 | Avg Reward: 0.576 | Avg Steps: 35.0 | Success Rate: 57.6% | Epsilon: 0.010\n",
            "Episode  3000 | Avg Reward: 0.624 | Avg Steps: 37.2 | Success Rate: 62.4% | Epsilon: 0.010\n",
            "Episode  3000 | Avg Reward: 0.624 | Avg Steps: 37.2 | Success Rate: 62.4% | Epsilon: 0.010\n",
            "Episode  4000 | Avg Reward: 0.660 | Avg Steps: 38.1 | Success Rate: 66.0% | Epsilon: 0.010\n",
            "Episode  4000 | Avg Reward: 0.660 | Avg Steps: 38.1 | Success Rate: 66.0% | Epsilon: 0.010\n",
            "Episode  5000 | Avg Reward: 0.664 | Avg Steps: 39.3 | Success Rate: 66.4% | Epsilon: 0.010\n",
            "Episode  5000 | Avg Reward: 0.664 | Avg Steps: 39.3 | Success Rate: 66.4% | Epsilon: 0.010\n",
            "Episode  6000 | Avg Reward: 0.627 | Avg Steps: 37.4 | Success Rate: 62.7% | Epsilon: 0.010\n",
            "Episode  6000 | Avg Reward: 0.627 | Avg Steps: 37.4 | Success Rate: 62.7% | Epsilon: 0.010\n",
            "Episode  7000 | Avg Reward: 0.653 | Avg Steps: 38.8 | Success Rate: 65.3% | Epsilon: 0.010\n",
            "Episode  7000 | Avg Reward: 0.653 | Avg Steps: 38.8 | Success Rate: 65.3% | Epsilon: 0.010\n",
            "Episode  8000 | Avg Reward: 0.663 | Avg Steps: 37.3 | Success Rate: 66.3% | Epsilon: 0.010\n",
            "Episode  8000 | Avg Reward: 0.663 | Avg Steps: 37.3 | Success Rate: 66.3% | Epsilon: 0.010\n",
            "Episode  9000 | Avg Reward: 0.650 | Avg Steps: 39.4 | Success Rate: 65.0% | Epsilon: 0.010\n",
            "Episode  9000 | Avg Reward: 0.650 | Avg Steps: 39.4 | Success Rate: 65.0% | Epsilon: 0.010\n",
            "Episode 10000 | Avg Reward: 0.671 | Avg Steps: 39.2 | Success Rate: 67.1% | Epsilon: 0.010\n",
            "============================================================\n",
            "TRAINING COMPLETE\n",
            "============================================================\n",
            "\n",
            "Final Statistics:\n",
            "  Total episodes: 10000\n",
            "  Final epsilon: 0.0100\n",
            "  Success rate (last 1000): 67.1%\n",
            "  Avg reward (last 1000): 0.671\n",
            "  Avg steps (last 1000): 39.2\n",
            "Episode 10000 | Avg Reward: 0.671 | Avg Steps: 39.2 | Success Rate: 67.1% | Epsilon: 0.010\n",
            "============================================================\n",
            "TRAINING COMPLETE\n",
            "============================================================\n",
            "\n",
            "Final Statistics:\n",
            "  Total episodes: 10000\n",
            "  Final epsilon: 0.0100\n",
            "  Success rate (last 1000): 67.1%\n",
            "  Avg reward (last 1000): 0.671\n",
            "  Avg steps (last 1000): 39.2\n"
          ]
        }
      ],
      "source": [
        "# Train the agent for 10,000 episodes\n",
        "print(\"Starting training on 4Ã—4 map...\")\n",
        "print(\"This may take 1-2 minutes...\\n\")\n",
        "\n",
        "rewards, steps = train_agent(agent, \n",
        "                            env,\n",
        "                            num_episodes=10000, \n",
        "                            max_steps=100)\n",
        "\n",
        "print(f\"\\nFinal Statistics:\")\n",
        "print(f\"  Total episodes: {len(rewards)}\")\n",
        "print(f\"  Final epsilon: {agent.epsilon:.4f}\")\n",
        "print(f\"  Success rate (last 1000): {np.sum(rewards[-1000:]) / 1000 * 100:.1f}%\")\n",
        "print(f\"  Avg reward (last 1000): {np.mean(rewards[-1000:]):.3f}\")\n",
        "print(f\"  Avg steps (last 1000): {np.mean(steps[-1000:]):.1f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "UNDERSTANDING THE RESULTS\n",
            "============================================================\n",
            "\n",
            " Success Rate: 67.1%\n",
            "   â†’ Agent reaches goal in 67 out of 100 attempts\n",
            "   â†’ Good performance on slippery floor (actions only work 33% as intended)\n",
            "\n",
            " Average Steps: 39.2\n",
            "   â†’ Takes many steps because robot constantly slips\n",
            "   â†’ Must correct path repeatedly (like walking on ice)\n",
            "   â†’ Optimal path is ~6 steps, but slipping adds ~33 extra steps\n",
            "\n",
            " Final Epsilon: 0.0100 (1.0%)\n",
            "   â†’ Started at 1.0 (100% random exploration)\n",
            "   â†’ Now at 0.01 (1% random, 99% using learned Q-table)\n",
            "   â†’ Agent mostly exploits learned knowledge\n",
            "\n",
            " Why Not 100% Success?\n",
            "   â†’ Slippery floor introduces unavoidable randomness\n",
            "   â†’ Even perfect policy can't prevent all bad luck (slipping into holes)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Dynamic results summary using actual training data\n",
        "success_rate = np.sum(rewards[-1000:]) / 1000 * 100\n",
        "avg_steps = np.mean(steps[-1000:])\n",
        "final_epsilon = agent.epsilon\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"UNDERSTANDING THE RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\n Success Rate: {success_rate:.1f}%\")\n",
        "print(f\"   â†’ Agent reaches goal in {int(success_rate)} out of 100 attempts\")\n",
        "print(f\"   â†’ Good performance on slippery floor (actions only work 33% as intended)\")\n",
        "\n",
        "print(f\"\\n Average Steps: {avg_steps:.1f}\")\n",
        "print(f\"   â†’ Takes many steps because robot constantly slips\")\n",
        "print(f\"   â†’ Must correct path repeatedly (like walking on ice)\")\n",
        "print(f\"   â†’ Optimal path is ~6 steps, but slipping adds ~{avg_steps - 6:.0f} extra steps\")\n",
        "\n",
        "print(f\"\\n Final Epsilon: {final_epsilon:.4f} ({final_epsilon * 100:.1f}%)\")\n",
        "print(f\"   â†’ Started at 1.0 (100% random exploration)\")\n",
        "print(f\"   â†’ Now at {final_epsilon:.2f} ({final_epsilon * 100:.0f}% random, {(1-final_epsilon) * 100:.0f}% using learned Q-table)\")\n",
        "print(f\"   â†’ Agent mostly exploits learned knowledge\")\n",
        "\n",
        "print(f\"\\n Why Not 100% Success?\")\n",
        "print(f\"   â†’ Slippery floor introduces unavoidable randomness\")\n",
        "print(f\"   â†’ Even perfect policy can't prevent all bad luck (slipping into holes)\")\n",
        "print(\"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
